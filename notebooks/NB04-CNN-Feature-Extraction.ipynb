{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB04 — CNN Feature Extraction\n",
    "\n",
    "**Goal**: Convert every sampled frame from **NB03** into a high-dimensional visual embedding using a frozen CNN backbone.\n",
    "\n",
    "### Why Feature Extraction?\n",
    "1. **Efficiency**: Running heavy CNNs (MobileNetV3) during every epoch of temporal model training (BiLSTM/Transformer) is computationally prohibitive. Pre-calculating them once saves hours of GPU time.\n",
    "2. **Standardization**: Ensures that both the BiLSTM and Transformer see the exact same visual inputs.\n",
    "3. **Focus**: Allows Phase 3 models to focus purely on temporal importance rather than pixel-level visual recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:43:35.414233Z",
     "iopub.status.busy": "2025-12-21T19:43:35.413957Z",
     "iopub.status.idle": "2025-12-21T19:43:42.872324Z",
     "shell.execute_reply": "2025-12-21T19:43:42.871648Z",
     "shell.execute_reply.started": "2025-12-21T19:43:35.414208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Environment detection\n",
    "IS_KAGGLE = Path(\"/kaggle/input\").exists()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # UPDATED: Exact Kaggle paths per user specification\n",
    "    FRAME_INDEX_PATH = Path(\"/kaggle/input/tvsum-frame-index/tvsum_frame_index.parquet\")\n",
    "    VIDEO_INDEX_PATH = Path(\"/kaggle/input/tvsum-index/tvsum_index.csv\")\n",
    "    # Note: NB04 will use the 'video_path' from tvsum_index.csv directly.\n",
    "else:\n",
    "    FRAME_INDEX_PATH = Path(\"data/processed/tvsum_frame_index.parquet\")\n",
    "    VIDEO_INDEX_PATH = Path(\"data/processed/tvsum_index.csv\")\n",
    "\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Metadata Preparation\n",
    "\n",
    "We merge `video_path` and `fps` from the master index into our sampling list to ensure the Dataset has everything it needs for accurate frame seeking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:43:42.874039Z",
     "iopub.status.busy": "2025-12-21T19:43:42.873725Z",
     "iopub.status.idle": "2025-12-21T19:43:43.078664Z",
     "shell.execute_reply": "2025-12-21T19:43:43.077940Z",
     "shell.execute_reply.started": "2025-12-21T19:43:42.874016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to process 25190 frames across 50 videos.\n"
     ]
    }
   ],
   "source": [
    "# Load indices\n",
    "frame_df = pd.read_parquet(FRAME_INDEX_PATH)\n",
    "video_df = pd.read_csv(VIDEO_INDEX_PATH)\n",
    "\n",
    "# Merge verification\n",
    "merged_df = frame_df.merge(video_df[['video_id', 'video_path', 'fps']], on='video_id', how='left')\n",
    "\n",
    "# Final check for missing paths\n",
    "missing_count = merged_df['video_path'].isna().sum()\n",
    "if missing_count > 0:\n",
    "    raise ValueError(f\"Found {missing_count} frames with no matching video path. Ensure NB02 was run correctly.\")\n",
    "\n",
    "print(f\"Ready to process {len(merged_df)} frames across {merged_df['video_id'].nunique()} videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Initialization (Frozen MobileNetV3)\n",
    "\n",
    "We use **MobileNetV3-Large** for its excellent balance of semantic richness and speed. We remove the classification head to get raw GAP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:43:43.079884Z",
     "iopub.status.busy": "2025-12-21T19:43:43.079621Z",
     "iopub.status.idle": "2025-12-21T19:43:43.545989Z",
     "shell.execute_reply": "2025-12-21T19:43:43.545351Z",
     "shell.execute_reply.started": "2025-12-21T19:43:43.079856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 178MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor ready. Feature dimension: 960\n"
     ]
    }
   ],
   "source": [
    "def get_feature_extractor():\n",
    "    # Load pre-trained model\n",
    "    model = models.mobilenet_v3_large(weights=\"IMAGENET1K_V1\")\n",
    "    \n",
    "    # Identity layer removes the final classifier but keeps the GAP output\n",
    "    model.classifier = nn.Identity()\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_feature_extractor()\n",
    "FEATURE_DIM = 960  # Default for MobileNetV3-Large architecture\n",
    "print(f\"Extractor ready. Feature dimension: {FEATURE_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. High-Resolution Data Pipe\n",
    "\n",
    "To solve the temporal alignment bug, we calculate `native_idx = timestamp * fps`. This ensures OpenCV seeks to the exact frame associated with our ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:43:43.547871Z",
     "iopub.status.busy": "2025-12-21T19:43:43.547654Z",
     "iopub.status.idle": "2025-12-21T19:43:43.556030Z",
     "shell.execute_reply": "2025-12-21T19:43:43.555259Z",
     "shell.execute_reply.started": "2025-12-21T19:43:43.547851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized. Sequential extraction (shuffle=False) is ACTIVE.\n"
     ]
    }
   ],
   "source": [
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = row['video_path']\n",
    "        timestamp = row['timestamp_sec']\n",
    "        fps = row['fps']\n",
    "        \n",
    "        # FIX: Calculate native frame index from timestamp to ensure accurate seeking\n",
    "        native_idx = int(round(timestamp * fps))\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, native_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            # Fallback: Create black frame if read fails to prevent crash\n",
    "            frame = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "            \n",
    "        # Convert BGR (OpenCV) to RGB (PIL/Torchvision)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img\n",
    "\n",
    "# Standard ImageNet transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = VideoFrameDataset(merged_df, transform=preprocess)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Pipeline initialized. Sequential extraction (shuffle=False) is ACTIVE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Extraction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T19:43:43.557258Z",
     "iopub.status.busy": "2025-12-21T19:43:43.557040Z",
     "iopub.status.idle": "2025-12-21T20:10:39.584526Z",
     "shell.execute_reply": "2025-12-21T20:10:39.583900Z",
     "shell.execute_reply.started": "2025-12-21T19:43:43.557237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12901bbe8c447a08d3ee28b58849fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Shape: (25190, 960)\n"
     ]
    }
   ],
   "source": [
    "num_frames = len(merged_df)\n",
    "all_features = np.zeros((num_frames, FEATURE_DIM), dtype=np.float32)\n",
    "\n",
    "start_ptr = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(loader)):\n",
    "        batch = batch.to(DEVICE)\n",
    "        features = model(batch)\n",
    "        \n",
    "        # Flatten and store\n",
    "        features_np = features.cpu().numpy()\n",
    "        batch_size = features_np.shape[0]\n",
    "        \n",
    "        all_features[start_ptr:start_ptr + batch_size] = features_np\n",
    "        start_ptr += batch_size\n",
    "\n",
    "print(f\"Extraction complete. Shape: {all_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Serialization & Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T20:17:52.663326Z",
     "iopub.status.busy": "2025-12-21T20:17:52.662615Z",
     "iopub.status.idle": "2025-12-21T20:17:52.718575Z",
     "shell.execute_reply": "2025-12-21T20:17:52.717835Z",
     "shell.execute_reply.started": "2025-12-21T20:17:52.663295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to data/processed/tvsum_features.npy\n",
      "NB04 complete. Ready for Phase 3 Modeling.\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURE_SAVE_PATH = PROCESSED_DIR / \"tvsum_features.npy\"\n",
    "MANIFEST_SAVE_PATH = PROCESSED_DIR / \"feature_manifest.json\"\n",
    "\n",
    "# Save array\n",
    "np.save(FEATURE_SAVE_PATH, all_features)\n",
    "\n",
    "# Save metadata\n",
    "manifest = {\n",
    "    \"backbone\": \"MobileNetV3-Large\",\n",
    "    \"weights\": \"IMAGENET1K_V1\",\n",
    "    \"feature_dim\": FEATURE_DIM,\n",
    "    \"total_frames\": num_frames,\n",
    "    \"alignment_verified\": True,\n",
    "    \"order\": \"sequential_per_frame_index\"\n",
    "}\n",
    "\n",
    "with open(MANIFEST_SAVE_PATH, 'w') as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "\n",
    "print(f\"Features saved to {FEATURE_SAVE_PATH}\")\n",
    "print(\"NB04 complete. Ready for Phase 3 Modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7643666,
     "sourceId": 12137468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9090657,
     "sourceId": 14248099,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9090660,
     "sourceId": 14248102,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
